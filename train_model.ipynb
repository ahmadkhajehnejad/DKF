{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from model import Model\n",
    "import os\n",
    "from pykalman import KalmanFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data.txt') as f:\n",
    "    first = f.readline()\n",
    "    first = first.strip('\\n')\n",
    "    temp = first.split(' ')\n",
    "    T = int(temp[0])\n",
    "    o_dim = int(temp[1])\n",
    "    s_dim = int(temp[2])\n",
    "    o_matrix = np.zeros((T, o_dim), np.float32)\n",
    "    for i in range(T):\n",
    "        temp = f.readline().strip('\\n').split(' ')\n",
    "        for j in range(s_dim):\n",
    "            o_matrix[i,j] = float(temp[j])\n",
    "    s_matrix = np.zeros((T, s_dim), np.float32)\n",
    "    for i in range(T):\n",
    "        temp = f.readline().strip('\\n').split(' ')\n",
    "        for j in range(o_dim):\n",
    "            s_matrix[i,j] = float(temp[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_dim = 10\n",
    "train_data = np.zeros((int(T/2), 3*s_dim), np.float32)\n",
    "for i in range(int(T/2)):\n",
    "    train_data[i, :] = np.concatenate((s_matrix[i, :], s_matrix[i+1, :], o_matrix[i+1, :]), axis=0)\n",
    "test_data = np.zeros((int(T/2) - 1, 3*s_dim), np.float32)\n",
    "for i in range(int(T/2) - 1):\n",
    "    test_data[i, :] = np.concatenate((s_matrix[int(T/2) + i, :], s_matrix[int(T/2) + 1 + i, :], o_matrix[int(T/2) + 1 + i, :]),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "    def random_batch(self, batch_size):\n",
    "        index = np.random.choice(np.arange(len(self.train)),batch_size, False)\n",
    "        return self.train[index,:]\n",
    "dataset = Dataset(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = './log/'\n",
    "os.popen('rm '+log_dir+'*')\n",
    "minibatch_size = 256\n",
    "model = Model(s_dim, h_dim, minibatch_size, 1e-4, log_dir)\n",
    "iteration = 300\n",
    "for epoch in range(85):\n",
    "    reconstruction_loss_train, likelihood_train, classify_loss_train = 0., 0., 0.\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    # in each epoch 500 iterations\n",
    "    for i in range(iteration):\n",
    "        reconstruction_loss_value, likelihood_value, classify_loss_value, summary = \\\n",
    "                model.update_params(dataset.random_batch(minibatch_size))\n",
    "            \n",
    "        reconstruction_loss_train += reconstruction_loss_value\n",
    "        likelihood_train += likelihood_value\n",
    "        classify_loss_train += classify_loss_value\n",
    "        model.train_writer.add_summary(summary, global_step.eval(model.sess))\n",
    "    \n",
    "    reconstruction_loss_train = reconstruction_loss_train / (iteration)\n",
    "    likelihood_train = -likelihood_train / (iteration)\n",
    "    classify_loss_train = classify_loss_train / (iteration)\n",
    "    \n",
    "\n",
    "    print(\"step: {},\\trecons loss: {:.4f},\\tlikelihood: {:.4f},\\tclass loss: {:.4f}\".format(global_step.eval(model.sess),\n",
    "            reconstruction_loss_train, likelihood_train, classify_loss_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_s_p = np.asarray(model.sess.run([model.s_t_p], {model.input_tensor: train_data[train_data.shape[0]-2 : ,:]}))[0][1,:]\n",
    "\n",
    "a_2, sig_2, a_3, sig_3 = model.sess.run([model.a_2, model.sigma_2, model.a_3, model.sigma_3])\n",
    "kf = KalmanFilter(initial_state_mean = np.transpose(np.matmul(last_s_p,a_2)), \\\n",
    "                  initial_state_covariance = sig_2, \\\n",
    "                  transition_matrices = np.transpose(a_2), \\\n",
    "                  transition_covariance = sig_2, \\\n",
    "                  observation_matrices = np.transpose(a_3),\\\n",
    "                  observation_covariance = sig_3)\n",
    "est_o_t_p = model.sess.run(model.o_t_p, {model.input_tensor: test_data[:1000,]})\n",
    "measurements = np.asarray(est_o_t_p)\n",
    "(est_s_t_p, est_s_t_p_covariances) = kf.filter(measurements)\n",
    "est_s_t = model.decode_s_t_p(est_s_t_p)\n",
    "print(np.mean(np.linalg.norm(est_s_t - test_data[:1000, s_dim : 2*s_dim] , axis = 1)), end = '')\n",
    "print(' / ', end = '')\n",
    "print(np.mean(np.linalg.norm(test_data[:1000, s_dim : 2*s_dim], axis = 1)))\n",
    "print('mean consecutive diff: ', end='')\n",
    "print(np.mean(np.linalg.norm(test_data[1:1000, s_dim : 2*s_dim] - test_data[:999, s_dim : 2*s_dim], axis = 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
