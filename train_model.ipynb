{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from model import Model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data.txt') as f:\n",
    "    first = f.readline()\n",
    "    first = first.strip('\\n')\n",
    "    temp = first.split(' ')\n",
    "    T = int(temp[0])\n",
    "    o_dim = int(temp[1])\n",
    "    s_dim = int(temp[2])\n",
    "    o_matrix = np.zeros((T, o_dim), np.float32)\n",
    "    for i in range(T):\n",
    "        temp = f.readline().strip('\\n').split(' ')\n",
    "        for j in range(s_dim):\n",
    "            o_matrix[i,j] = float(temp[j])\n",
    "    s_matrix = np.zeros((T, s_dim), np.float32)\n",
    "    for i in range(T):\n",
    "        temp = f.readline().strip('\\n').split(' ')\n",
    "        for j in range(o_dim):\n",
    "            s_matrix[i,j] = float(temp[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_dim = 10\n",
    "train_data = np.zeros((int(T/2), 3*s_dim), np.float32)\n",
    "for i in range(int(T/2)):\n",
    "    train_data[i, :] = np.concatenate((s_matrix[i, :], s_matrix[i+1, :], o_matrix[i+1, :]), axis=0)\n",
    "test_data = np.zeros((int(T/2) - 1, 3*s_dim), np.float32)\n",
    "for i in range(int(T/2) - 1):\n",
    "    test_data[i, :] = np.concatenate((s_matrix[int(T/2) + i, :], s_matrix[int(T/2) + 1 + i, :], o_matrix[int(T/2) + 1 + i, :]),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "    def random_batch(self, batch_size):\n",
    "        index = np.random.choice(np.arange(len(self.train)),batch_size, False)\n",
    "        return self.train[index,:]\n",
    "dataset = Dataset(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = './log/'\n",
    "os.popen('rm '+log_dir+'*')\n",
    "minibatch_size = 256\n",
    "model = Model(s_dim, h_dim, minibatch_size, 1e-4, log_dir)\n",
    "iteration = 300\n",
    "for epoch in range(500):\n",
    "    reconstruction_loss_train, classify_loss_train = 0., 0.\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    # in each epoch 500 iterations\n",
    "    for i in range(iteration):\n",
    "        reconstruction_loss_value, classify_loss_value, summary = \\\n",
    "                model.update_params(dataset.random_batch(minibatch_size))\n",
    "        classify_loss_train += classify_loss_value\n",
    "        reconstruction_loss_train += reconstruction_loss_value\n",
    "        model.train_writer.add_summary(summary, global_step.eval(model.sess))\n",
    "    \n",
    "    classify_loss_train = classify_loss_train / (iteration)\n",
    "    reconstruction_loss_train = reconstruction_loss_train / (iteration)\n",
    "\n",
    "    print(\"step: {},\\trecons loss: {:.4f},\\tclass loss: {:.4f}\".format(global_step.eval(model.sess),\n",
    "            reconstruction_loss_train, classify_loss_train))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
